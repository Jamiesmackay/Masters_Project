{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TIME_STEPS = 60  # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 3  # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"P_avg\"\n",
    "EPOCHS = 10  # how many passes through our data\n",
    "SEQ_LEN = 60\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n",
    "NAME = f\"{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\"  # a unique name for the model\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('Updated_Basic_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking if any null values are present\n",
      " Wa_avg     0\n",
      "Wa_min     0\n",
      "Wa_max     0\n",
      "Ws1_avg    0\n",
      "Ws1_min    0\n",
      "Ws1_max    0\n",
      "Ws2_avg    0\n",
      "Ws2_min    0\n",
      "Ws2_max    0\n",
      "Ws_avg     0\n",
      "Ws_min     0\n",
      "Ws_max     0\n",
      "Rs_avg     0\n",
      "Rs_min     0\n",
      "Rs_max     0\n",
      "P_avg      0\n",
      "P_min      0\n",
      "P_max      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "main_df_R80711 = main_df[main_df.Wind_turbine_name == 'R80711']\n",
    "main_df_R80711 = main_df_R80711.reset_index()[['Wa_avg', 'Wa_min', 'Wa_max',\n",
    "                                               'Ws1_avg', 'Ws1_min', 'Ws1_max', 'Ws2_avg', 'Ws2_min', 'Ws2_max', 'Ws_avg', 'Ws_min', 'Ws_max',\n",
    "                                               'Rs_avg', 'Rs_min', 'Rs_max', \n",
    "                                               'P_avg',  'P_min', 'P_max']]\n",
    "\n",
    "main_df_R80721 = main_df[main_df.Wind_turbine_name == 'R80721']\n",
    "main_df_R80721 = main_df_R80721.reset_index()[['Wa_avg', 'Wa_min', 'Wa_max',\n",
    "                                               'Ws1_avg', 'Ws1_min', 'Ws1_max', 'Ws2_avg', 'Ws2_min', 'Ws2_max', 'Ws_avg', 'Ws_min', 'Ws_max',\n",
    "                                               'Rs_avg', 'Rs_min', 'Rs_max', \n",
    "                                               'P_avg',  'P_min', 'P_max']]\n",
    "\n",
    "main_df_R80736 = main_df[main_df.Wind_turbine_name == 'R80736']\n",
    "main_df_R80736 = main_df_R80736.reset_index()[['Wa_avg', 'Wa_min', 'Wa_max',\n",
    "                                               'Ws1_avg', 'Ws1_min', 'Ws1_max', 'Ws2_avg', 'Ws2_min', 'Ws2_max', 'Ws_avg', 'Ws_min', 'Ws_max',\n",
    "                                               'Rs_avg', 'Rs_min', 'Rs_max', \n",
    "                                               'P_avg',  'P_min', 'P_max']]\n",
    "\n",
    "main_df_R80790 = main_df[main_df.Wind_turbine_name == 'R80790']\n",
    "main_df_R80790 = main_df_R80790.reset_index()[['Wa_avg', 'Wa_min', 'Wa_max',\n",
    "                                               'Ws1_avg', 'Ws1_min', 'Ws1_max', 'Ws2_avg', 'Ws2_min', 'Ws2_max', 'Ws_avg', 'Ws_min', 'Ws_max',\n",
    "                                               'Rs_avg', 'Rs_min', 'Rs_max', \n",
    "                                               'P_avg',  'P_min', 'P_max']]\n",
    "\n",
    "print(\"checking if any null values are present\\n\", main_df_R80711.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    '''\n",
    "    Function that splits the dataframe into training and test data via an 80:20 split\n",
    "    '''\n",
    "    Index = sorted(data.index.values) \n",
    "    Last20pct = Index[-int(0.2*len(Index))]\n",
    "\n",
    "    validation_data = data[(data.index >= Last20pct)]\n",
    "    data = data[(data.index < Last20pct)]\n",
    "    return validation_data, data\n",
    "\n",
    "def scale_data(data,column,scaler):\n",
    "    '''\n",
    "    Column should be literal for funciton to work \n",
    "    '''\n",
    "    data_form = data[column].values.reshape((len(data), 1))\n",
    "    scaled_data = scaler.transform(data_form)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test train split using 20 80 in hopes of more better validation\n",
    "Test_R80711, Train_R80711 = train_test_split(main_df_R80711)\n",
    "#\n",
    "Test_R80721, Train_R80721 = train_test_split(main_df_R80721)\n",
    "#\n",
    "Test_R80736, Train_R80736 = train_test_split(main_df_R80736)\n",
    "#\n",
    "Test_R80790, Train_R80790 = train_test_split(main_df_R80790)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First fit the scalers to the data then transform the dat using said scalers using the scale_data function\n",
    "Wa_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "Scaler_fit_Wa_avg = Train_R80711['Wa_avg'].values.reshape(len(Train_R80711['Wa_avg']),1)\n",
    "Wa_scaler.fit(Scaler_fit_Wa_avg)\n",
    "Ws_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "Scaler_fit_Ws_avg = Train_R80711['Ws_avg'].values.reshape(len(Train_R80711['Ws_avg']),1)\n",
    "Ws_scaler.fit(Scaler_fit_Ws_avg)\n",
    "Rs_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "Scaler_fit_Rs_avg = Train_R80711['Rs_avg'].values.reshape(len(Train_R80711['Rs_avg']),1)\n",
    "Rs_scaler.fit(Scaler_fit_Rs_avg)\n",
    "P_scaler  = MinMaxScaler(feature_range=(0,1))\n",
    "Scaler_fit_P_avg = Train_R80711['P_avg'].values.reshape(len(Train_R80711['P_avg']),1)\n",
    "P_scaler.fit(Scaler_fit_P_avg)\n",
    "\n",
    "# Training Data scaled\n",
    "\n",
    "Scaled_Wa_avg_R80711 = scale_data(Train_R80711,'Wa_avg',Wa_scaler)\n",
    "Scaled_Wa_min_R80711 = scale_data(Train_R80711,'Wa_min',Wa_scaler)\n",
    "Scaled_Wa_max_R80711 = scale_data(Train_R80711,'Wa_max',Wa_scaler)\n",
    "Scaled_Ws1_avg_R80711 = scale_data(Train_R80711,'Ws1_avg',Ws_scaler)\n",
    "Scaled_Ws1_min_R80711 = scale_data(Train_R80711,'Ws1_min',Ws_scaler)\n",
    "Scaled_Ws1_max_R80711 = scale_data(Train_R80711,'Ws1_max',Ws_scaler)\n",
    "Scaled_Ws2_avg_R80711 = scale_data(Train_R80711,'Ws2_avg',Ws_scaler)\n",
    "Scaled_Ws2_min_R80711 = scale_data(Train_R80711,'Ws2_min',Ws_scaler)\n",
    "Scaled_Ws2_max_R80711 = scale_data(Train_R80711,'Ws2_max',Ws_scaler)\n",
    "Scaled_Ws_avg_R80711 = scale_data(Train_R80711,'Ws_avg',Ws_scaler)\n",
    "Scaled_Ws_min_R80711 = scale_data(Train_R80711,'Ws_min',Ws_scaler)\n",
    "Scaled_Ws_max_R80711 = scale_data(Train_R80711,'Ws_max',Ws_scaler)\n",
    "Scaled_Rs_avg_R80711 = scale_data(Train_R80711,'Rs_avg',Rs_scaler)\n",
    "Scaled_Rs_min_R80711 = scale_data(Train_R80711,'Rs_min',Rs_scaler)\n",
    "Scaled_Rs_max_R80711 = scale_data(Train_R80711,'Rs_max',Rs_scaler)\n",
    "Scaled_P_avg_R80711 = scale_data(Train_R80711,'P_avg',P_scaler)\n",
    "Scaled_P_min_R80711 = scale_data(Train_R80711,'P_min',P_scaler)\n",
    "Scaled_P_max_R80711 = scale_data(Train_R80711,'P_max',P_scaler)\n",
    "\n",
    "# Testing Data scaled\n",
    "\n",
    "Scaled_Wa_avg_R80711_Test = scale_data(Test_R80711,'Wa_avg',Wa_scaler)\n",
    "Scaled_Wa_min_R80711_Test = scale_data(Test_R80711,'Wa_min',Wa_scaler)\n",
    "Scaled_Wa_max_R80711_Test = scale_data(Test_R80711,'Wa_max',Wa_scaler)\n",
    "Scaled_Ws1_avg_R80711_Test = scale_data(Test_R80711,'Ws1_avg',Ws_scaler)\n",
    "Scaled_Ws1_min_R80711_Test = scale_data(Test_R80711,'Ws1_min',Ws_scaler)\n",
    "Scaled_Ws1_max_R80711_Test = scale_data(Test_R80711,'Ws1_max',Ws_scaler)\n",
    "Scaled_Ws2_avg_R80711_Test = scale_data(Test_R80711,'Ws2_avg',Ws_scaler)\n",
    "Scaled_Ws2_min_R80711_Test = scale_data(Test_R80711,'Ws2_min',Ws_scaler)\n",
    "Scaled_Ws2_max_R80711_Test = scale_data(Test_R80711,'Ws2_max',Ws_scaler)\n",
    "Scaled_Ws_avg_R80711_Test = scale_data(Test_R80711,'Ws_avg',Ws_scaler)\n",
    "Scaled_Ws_min_R80711_Test = scale_data(Test_R80711,'Ws_min',Ws_scaler)\n",
    "Scaled_Ws_max_R80711_Test = scale_data(Test_R80711,'Ws_max',Ws_scaler)\n",
    "Scaled_Rs_avg_R80711_Test = scale_data(Test_R80711,'Rs_avg',Rs_scaler)\n",
    "Scaled_Rs_min_R80711_Test = scale_data(Test_R80711,'Rs_min',Rs_scaler)\n",
    "Scaled_Rs_max_R80711_Test = scale_data(Test_R80711,'Rs_max',Rs_scaler)\n",
    "Scaled_P_avg_R80711_Test = scale_data(Test_R80711,'P_avg',P_scaler)\n",
    "Scaled_P_min_R80711_Test = scale_data(Test_R80711,'P_min',P_scaler)\n",
    "Scaled_P_max_R80711_Test = scale_data(Test_R80711,'P_max',P_scaler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132149, 60, 15) (132149,)\n"
     ]
    }
   ],
   "source": [
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "in_seq1 = Scaled_Wa_avg_R80711\n",
    "in_seq2 = Scaled_Wa_min_R80711\n",
    "in_seq3 = Scaled_Wa_max_R80711\n",
    "in_seq4 = Scaled_Ws1_avg_R80711\n",
    "in_seq5 = Scaled_Ws1_min_R80711\n",
    "in_seq6 = Scaled_Ws1_max_R80711\n",
    "in_seq7 = Scaled_Ws2_avg_R80711\n",
    "in_seq8 = Scaled_Ws2_min_R80711\n",
    "in_seq9 = Scaled_Ws2_max_R80711\n",
    "in_seq10 = Scaled_Ws_avg_R80711\n",
    "in_seq11 = Scaled_Ws_min_R80711\n",
    "in_seq12 = Scaled_Ws_max_R80711\n",
    "in_seq13 = Scaled_Rs_avg_R80711\n",
    "in_seq14 = Scaled_Rs_min_R80711\n",
    "in_seq15 = Scaled_Rs_max_R80711\n",
    "out_seq = Scaled_P_avg_R80711\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "in_seq3 = in_seq3.reshape((len(in_seq3), 1))\n",
    "in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
    "in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
    "in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
    "in_seq7 = in_seq7.reshape((len(in_seq7), 1))\n",
    "in_seq8 = in_seq8.reshape((len(in_seq8), 1))\n",
    "in_seq9 = in_seq9.reshape((len(in_seq9), 1))\n",
    "in_seq10 = in_seq10.reshape((len(in_seq10), 1))\n",
    "in_seq11 = in_seq11.reshape((len(in_seq11), 1))\n",
    "in_seq12 = in_seq12.reshape((len(in_seq12), 1))\n",
    "in_seq13 = in_seq13.reshape((len(in_seq13), 1))\n",
    "in_seq14 = in_seq14.reshape((len(in_seq14), 1))\n",
    "in_seq15 = in_seq15.reshape((len(in_seq15), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, in_seq3, in_seq4, in_seq5, in_seq6, in_seq7, in_seq8, in_seq9, in_seq10, \n",
    "                  in_seq11, in_seq12, in_seq13, in_seq14, in_seq15, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps_in = 60\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32993, 60, 15) (32993,)\n"
     ]
    }
   ],
   "source": [
    "# Test data sorted for model\n",
    "\n",
    "# define input sequence\n",
    "in_seq1_Test = Scaled_Wa_avg_R80711_Test\n",
    "in_seq2_Test = Scaled_Wa_min_R80711_Test\n",
    "in_seq3_Test = Scaled_Wa_max_R80711_Test\n",
    "in_seq4_Test = Scaled_Ws1_avg_R80711_Test\n",
    "in_seq5_Test = Scaled_Ws1_min_R80711_Test\n",
    "in_seq6_Test = Scaled_Ws1_max_R80711_Test\n",
    "in_seq7_Test = Scaled_Ws2_avg_R80711_Test\n",
    "in_seq8_Test = Scaled_Ws2_min_R80711_Test\n",
    "in_seq9_Test = Scaled_Ws2_max_R80711_Test\n",
    "in_seq10_Test = Scaled_Ws_avg_R80711_Test\n",
    "in_seq11_Test = Scaled_Ws_min_R80711_Test\n",
    "in_seq12_Test = Scaled_Ws_max_R80711_Test\n",
    "in_seq13_Test = Scaled_Rs_avg_R80711_Test\n",
    "in_seq14_Test = Scaled_Rs_min_R80711_Test\n",
    "in_seq15_Test = Scaled_Rs_max_R80711_Test\n",
    "out_seq_Test = Scaled_P_avg_R80711_Test\n",
    "\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1_Test = in_seq1_Test.reshape((len(in_seq1_Test), 1))\n",
    "in_seq2_Test = in_seq2_Test.reshape((len(in_seq2_Test), 1))\n",
    "in_seq3_Test = in_seq3_Test.reshape((len(in_seq3_Test), 1))\n",
    "in_seq4_Test = in_seq4_Test.reshape((len(in_seq4_Test), 1))\n",
    "in_seq5_Test = in_seq5_Test.reshape((len(in_seq5_Test), 1))\n",
    "in_seq6_Test = in_seq6_Test.reshape((len(in_seq6_Test), 1))\n",
    "in_seq7_Test = in_seq7_Test.reshape((len(in_seq7_Test), 1))\n",
    "in_seq8_Test = in_seq8_Test.reshape((len(in_seq8_Test), 1))\n",
    "in_seq9_Test = in_seq9_Test.reshape((len(in_seq9_Test), 1))\n",
    "in_seq10_Test = in_seq10_Test.reshape((len(in_seq10_Test), 1))\n",
    "in_seq11_Test = in_seq11_Test.reshape((len(in_seq11_Test), 1))\n",
    "in_seq12_Test = in_seq12_Test.reshape((len(in_seq12_Test), 1))\n",
    "in_seq13_Test = in_seq13_Test.reshape((len(in_seq13_Test), 1))\n",
    "in_seq14_Test = in_seq14_Test.reshape((len(in_seq14_Test), 1))\n",
    "in_seq15_Test = in_seq15_Test.reshape((len(in_seq15_Test), 1))\n",
    "out_seq_Test = out_seq_Test.reshape((len(out_seq_Test), 1))\n",
    "# horizontally stack columns\n",
    "dataset_Test = hstack((in_seq1_Test, in_seq2_Test, in_seq3_Test, in_seq4_Test, in_seq5_Test, in_seq6_Test, in_seq7_Test,\n",
    "                  in_seq8_Test, in_seq9_Test, in_seq10_Test, in_seq11_Test, in_seq12_Test, in_seq13_Test, in_seq14_Test,\n",
    "                  in_seq15_Test, out_seq_Test))\n",
    "\n",
    "X_Test, y_Test = split_sequences(dataset_Test, n_steps)\n",
    "print(X_Test.shape, y_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(1024, input_shape=(n_steps_in, n_features), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1,activation = 'linear'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001,decay=1e-6)\n",
    "model.compile(optimizer=opt, loss='mae', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105719 samples, validate on 26430 samples\n",
      "Epoch 1/10\n",
      "105719/105719 [==============================] - 376s 4ms/sample - loss: 0.0942 - acc: 1.8918e-05 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "105719/105719 [==============================] - 374s 4ms/sample - loss: 0.0435 - acc: 1.8918e-05 - val_loss: 0.0510 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "105719/105719 [==============================] - 374s 4ms/sample - loss: 0.0358 - acc: 1.8918e-05 - val_loss: 0.0484 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "105719/105719 [==============================] - 375s 4ms/sample - loss: 0.0311 - acc: 1.8918e-05 - val_loss: 0.0298 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "105719/105719 [==============================] - 375s 4ms/sample - loss: 0.0284 - acc: 1.8918e-05 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "105719/105719 [==============================] - 375s 4ms/sample - loss: 0.0265 - acc: 1.8918e-05 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "105719/105719 [==============================] - 375s 4ms/sample - loss: 0.0258 - acc: 1.8918e-05 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "105719/105719 [==============================] - 376s 4ms/sample - loss: 0.0251 - acc: 1.8918e-05 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "105719/105719 [==============================] - 377s 4ms/sample - loss: 0.0249 - acc: 1.8918e-05 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "105719/105719 [==============================] - 378s 4ms/sample - loss: 0.0243 - acc: 1.8918e-05 - val_loss: 0.0198 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "            # Train model\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    validation_split=0.2,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_16 (CuDNNLSTM)    (None, 60, 1024)          4263936   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 60, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_19 (B (None, 60, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_17 (CuDNNLSTM)    (None, 60, 512)           3149824   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 60, 512)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_20 (B (None, 60, 512)           2048      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)    (None, 60, 256)           788480    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 60, 256)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_21 (B (None, 60, 256)           1024      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)    (None, 128)               197632    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,415,873\n",
      "Trainable params: 8,412,033\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # demonstrate prediction\n",
    "ypred = [None]*len(X_Test)\n",
    "\n",
    "for i in range(len(X_Test)):\n",
    "        x_input = X_Test[i]\n",
    "        x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        ypred[i] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28.44207403]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred  = np.asarray(ypred)\n",
    "ypred  = ypred.reshape((len(ypred), 1))\n",
    "y_Test = np.asarray(y_Test)\n",
    "y_Test = y_Test.reshape((len(y_Test), 1))\n",
    "\n",
    "Mean_Abs_Error = sum(abs(ypred - y_Test))/len(y_Test)\n",
    "Mean_Abs_Error = Mean_Abs_Error.reshape((len(Mean_Abs_Error),1))\n",
    "P_scaler.inverse_transform(Mean_Abs_Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2547651054796845"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28.44207403/(684.15002-142.88750)*100 # Relating the MAE to the IQR for better understanding relative to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[10 15]]] => [25]\n",
      "[[[20 25]]] => [45]\n",
      "[[[30 35]]] => [65]\n",
      "[[[40 45]]] => [85]\n",
      "[[[50 55]]] => [105]\n",
      "[[[60 65]]] => [125]\n",
      "[[[70 75]]] => [145]\n",
      "[[[80 85]]] => [165]\n",
      "[[[90 95]]] => [185]\n",
      "[[[100 105]]] => [205]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
